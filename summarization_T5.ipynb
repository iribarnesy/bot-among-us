{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "summarization_T5.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a5021d0f563546c18e55a9bac0a14a92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_ac6016d0a3884d8bb45156217f7a357e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_fe234a809f5341f1a832f88ced90e0fd",
              "IPY_MODEL_abed7f80048b495a8a1b9287edb08cf1"
            ]
          }
        },
        "ac6016d0a3884d8bb45156217f7a357e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fe234a809f5341f1a832f88ced90e0fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_787bde7b73c04ca485a7d3ffac6dee48",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 791656,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 791656,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_96cdfaae8c92406e900a0a5b48f20e0f"
          }
        },
        "abed7f80048b495a8a1b9287edb08cf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ef5716710a9b4a68b4cc91ad5c581dbf",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 792k/792k [00:01&lt;00:00, 601kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6e37979905c749cd9c97efd250859922"
          }
        },
        "787bde7b73c04ca485a7d3ffac6dee48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "96cdfaae8c92406e900a0a5b48f20e0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ef5716710a9b4a68b4cc91ad5c581dbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6e37979905c749cd9c97efd250859922": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "af8b524fc9f94a2ca9e83ad3abbc8da8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_23d9362b100f4496afcd9a754c465dd0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_bbef1db670e341dba6f17abdf2fe7949",
              "IPY_MODEL_65dc0d3d7fe844bc9e9df4a441fc200d"
            ]
          }
        },
        "23d9362b100f4496afcd9a754c465dd0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bbef1db670e341dba6f17abdf2fe7949": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_cc053aa33e30447cb16c7a0af0699dd0",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1389353,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1389353,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a9eb0c685a3749318ac8a4ee67fe774b"
          }
        },
        "65dc0d3d7fe844bc9e9df4a441fc200d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_88c2715850dd4c6890105b730409df49",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.39M/1.39M [00:02&lt;00:00, 644kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5097d2422530404f87dd13691cfa9bfb"
          }
        },
        "cc053aa33e30447cb16c7a0af0699dd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a9eb0c685a3749318ac8a4ee67fe774b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "88c2715850dd4c6890105b730409df49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5097d2422530404f87dd13691cfa9bfb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsNrgJ0W-uog"
      },
      "source": [
        "https://huggingface.co/mrm8488/t5-base-finetuned-summarize-news?text=Le+jaune+%C3%A9tait+caf%C3%A9t%C3%A9ria.+J%27ai+remis+les+lumi%C3%A8res.+J%27ai+vu+le+noir+vent.+Je+suis+all%C3%A9+admin.+J%27ai+crois%C3%A9+le+rouge+weapons.+J%27ai+crois%C3%A9+le+vert.+J%27ai+fait+les+fils.+J%27ai+tu%C3%A9+le+bleu.+Je+suis+all%C3%A9+navigation.+Je+suis+all%C3%A9+medbay.+J%27ai+tu%C3%A9+le+rouge."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvnIcYNd-v_l"
      },
      "source": [
        "https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_summarization_wandb.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDlnDpRJAd5n",
        "outputId": "0fd39af1-a88d-4aed-e283-14e6487f70de"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DJlpcqaI4Ei",
        "outputId": "eb1ba364-250f-4cc2-e4be-ed54c9bb8031"
      },
      "source": [
        "!pip install --upgrade pip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pip\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/6f/43037c7bcc8bd8ba7c9074256b1a11596daa15555808ec748048c1507f08/pip-21.1.1-py3-none-any.whl (1.5MB)\n",
            "\r\u001b[K     |▏                               | 10kB 21.4MB/s eta 0:00:01\r\u001b[K     |▍                               | 20kB 28.8MB/s eta 0:00:01\r\u001b[K     |▋                               | 30kB 21.4MB/s eta 0:00:01\r\u001b[K     |▉                               | 40kB 16.5MB/s eta 0:00:01\r\u001b[K     |█                               | 51kB 8.6MB/s eta 0:00:01\r\u001b[K     |█▎                              | 61kB 7.8MB/s eta 0:00:01\r\u001b[K     |█▌                              | 71kB 8.9MB/s eta 0:00:01\r\u001b[K     |█▊                              | 81kB 9.8MB/s eta 0:00:01\r\u001b[K     |██                              | 92kB 10.3MB/s eta 0:00:01\r\u001b[K     |██▏                             | 102kB 8.4MB/s eta 0:00:01\r\u001b[K     |██▎                             | 112kB 8.4MB/s eta 0:00:01\r\u001b[K     |██▌                             | 122kB 8.4MB/s eta 0:00:01\r\u001b[K     |██▊                             | 133kB 8.4MB/s eta 0:00:01\r\u001b[K     |███                             | 143kB 8.4MB/s eta 0:00:01\r\u001b[K     |███▏                            | 153kB 8.4MB/s eta 0:00:01\r\u001b[K     |███▍                            | 163kB 8.4MB/s eta 0:00:01\r\u001b[K     |███▋                            | 174kB 8.4MB/s eta 0:00:01\r\u001b[K     |███▉                            | 184kB 8.4MB/s eta 0:00:01\r\u001b[K     |████                            | 194kB 8.4MB/s eta 0:00:01\r\u001b[K     |████▎                           | 204kB 8.4MB/s eta 0:00:01\r\u001b[K     |████▌                           | 215kB 8.4MB/s eta 0:00:01\r\u001b[K     |████▋                           | 225kB 8.4MB/s eta 0:00:01\r\u001b[K     |████▉                           | 235kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████                           | 245kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 256kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 266kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 276kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████                          | 286kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 296kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 307kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 317kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 327kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████                         | 337kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 348kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 358kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 368kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 378kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████                        | 389kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 399kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 409kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 419kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████                       | 430kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████                       | 440kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 450kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 460kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 471kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████                      | 481kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 491kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 501kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 512kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 522kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████                     | 532kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 542kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 552kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 563kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 573kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████                    | 583kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 593kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 604kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 614kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 624kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 634kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 645kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 655kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 665kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 675kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 686kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 696kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 706kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 716kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 727kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 737kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 747kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 757kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 768kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████                | 778kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 788kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 798kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 808kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 819kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 829kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 839kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 849kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 860kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 870kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 880kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 890kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 901kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 911kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 921kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 931kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 942kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 952kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 962kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 972kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 983kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 993kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 1.0MB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.0MB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 1.0MB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 1.0MB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 1.0MB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 1.1MB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.1MB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.1MB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.1MB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.1MB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.1MB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.1MB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 1.1MB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.1MB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.1MB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.2MB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.2MB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.2MB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.2MB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.2MB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.2MB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.2MB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.2MB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 1.2MB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 1.2MB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.3MB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 1.3MB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.3MB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.3MB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.3MB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.3MB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.3MB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.3MB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.3MB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.4MB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.4MB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.4MB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.4MB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.4MB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.4MB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.4MB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.4MB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 1.4MB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.4MB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.5MB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.5MB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.5MB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.5MB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.5MB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.5MB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.5MB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.5MB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.5MB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.5MB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.6MB 8.4MB/s \n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Found existing installation: pip 19.3.1\n",
            "    Uninstalling pip-19.3.1:\n",
            "      Successfully uninstalled pip-19.3.1\n",
            "Successfully installed pip-21.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_fvFR1A-u7d",
        "outputId": "ca2db62a-63b3-4d02-933c-df055ad6dca5"
      },
      "source": [
        "!pip install transformers[sentencepiece]\n",
        "!pip install wandb -q"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers[sentencepiece]\n",
            "  Downloading transformers-4.6.1-py3-none-any.whl (2.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 6.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (20.9)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (1.19.5)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 54.0 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub==0.0.8\n",
            "  Downloading huggingface_hub-0.0.8-py3-none-any.whl (34 kB)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 68.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (4.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (2019.12.20)\n",
            "Collecting sentencepiece==0.1.91\n",
            "  Downloading sentencepiece-0.1.91-cp37-cp37m-manylinux1_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 52.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers[sentencepiece]) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers[sentencepiece]) (3.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers[sentencepiece]) (2.4.7)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->transformers[sentencepiece]) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf->transformers[sentencepiece]) (56.1.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers[sentencepiece]) (8.0.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers[sentencepiece]) (1.0.1)\n",
            "Installing collected packages: tokenizers, sacremoses, huggingface-hub, transformers, sentencepiece\n",
            "Successfully installed huggingface-hub-0.0.8 sacremoses-0.0.45 sentencepiece-0.1.91 tokenizers-0.10.2 transformers-4.6.1\n",
            "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 8.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 131 kB 70.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 97 kB 7.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 166 kB 64.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.2 MB/s \n",
            "\u001b[?25h  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zz2ItOTEBx0"
      },
      "source": [
        "# Importing stock libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Importing the T5 modules from huggingface/transformers\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "# WandB – Import the wandb library\n",
        "import wandb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2sQVSm5EEjk",
        "outputId": "ff54285c-dba1-4b09-ce9c-5ce6f82a3b13"
      },
      "source": [
        "# Checking out the GPU we have access to. This is output is from the google colab version. \n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu May 20 22:59:59 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUSe63srEGvC"
      },
      "source": [
        "# # Setting up the device for GPU usage\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "\n",
        "# Preparing for TPU usage\n",
        "# import torch_xla\n",
        "# import torch_xla.core.xla_model as xm\n",
        "# device = xm.xla_device()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ec17RsFEESfe",
        "outputId": "d2cc7051-8178-4f57-b548-1d377daacdd1"
      },
      "source": [
        "# Login to wandb to log the model run and all the parameters\n",
        "!wandb login"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdenatflore\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GM9-WtY2FKeB"
      },
      "source": [
        "# Creating a custom dataset for reading the dataframe and loading it into the dataloader to pass it to the neural network at a later stage for finetuning the model and to prepare it for predictions\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataframe, tokenizer, source_len, summ_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.source_len = source_len\n",
        "        self.summ_len = summ_len\n",
        "        self.text = self.data.text\n",
        "        self.ctext = self.data.ctext\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        ctext = str(self.ctext[index])\n",
        "        ctext = ' '.join(ctext.split())\n",
        "\n",
        "        text = str(self.text[index])\n",
        "        text = ' '.join(text.split())\n",
        "\n",
        "        source = self.tokenizer.batch_encode_plus([ctext], max_length= self.source_len, pad_to_max_length=True,return_tensors='pt')\n",
        "        target = self.tokenizer.batch_encode_plus([text], max_length= self.summ_len, pad_to_max_length=True,return_tensors='pt')\n",
        "\n",
        "        source_ids = source['input_ids'].squeeze()\n",
        "        source_mask = source['attention_mask'].squeeze()\n",
        "        target_ids = target['input_ids'].squeeze()\n",
        "        target_mask = target['attention_mask'].squeeze()\n",
        "\n",
        "        return {\n",
        "            'source_ids': source_ids.to(dtype=torch.long), \n",
        "            'source_mask': source_mask.to(dtype=torch.long), \n",
        "            'target_ids': target_ids.to(dtype=torch.long),\n",
        "            'target_ids_y': target_ids.to(dtype=torch.long)\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ErzNPSBGCNo"
      },
      "source": [
        "\n",
        "# Creating the training function. This will be called in the main function. It is run depending on the epoch value.\n",
        "# The model is put into train mode and then we wnumerate over the training loader and passed to the defined network \n",
        "\n",
        "def train(epoch, tokenizer, model, device, loader, optimizer):\n",
        "    model.train()\n",
        "    for _,data in enumerate(loader, 0):\n",
        "        y = data['target_ids'].to(device, dtype = torch.long)\n",
        "        y_ids = y[:, :-1].contiguous()\n",
        "        lm_labels = y[:, 1:].clone().detach()\n",
        "        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n",
        "        ids = data['source_ids'].to(device, dtype = torch.long)\n",
        "        mask = data['source_mask'].to(device, dtype = torch.long)\n",
        "\n",
        "        outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, labels=lm_labels)\n",
        "        loss = outputs[0]\n",
        "        \n",
        "        if _%10 == 0:\n",
        "            wandb.log({\"Training Loss\": loss.item()})\n",
        "\n",
        "        if _%500==0:\n",
        "            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        # xm.optimizer_step(optimizer)\n",
        "        # xm.mark_step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDbNkU3LGXma"
      },
      "source": [
        "def validate(epoch, tokenizer, model, device, loader):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "    with torch.no_grad():\n",
        "        for _, data in enumerate(loader, 0):\n",
        "            y = data['target_ids'].to(device, dtype = torch.long)\n",
        "            ids = data['source_ids'].to(device, dtype = torch.long)\n",
        "            mask = data['source_mask'].to(device, dtype = torch.long)\n",
        "\n",
        "            generated_ids = model.generate(\n",
        "                input_ids = ids,\n",
        "                attention_mask = mask, \n",
        "                max_length=150, \n",
        "                # max_length=50,\n",
        "                num_beams=2,\n",
        "                repetition_penalty=2.5, \n",
        "                length_penalty=1.0, \n",
        "                early_stopping=True\n",
        "                )\n",
        "            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
        "            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n",
        "            if _%100==0:\n",
        "                print(f'Completed {_}')\n",
        "\n",
        "            predictions.extend(preds)\n",
        "            actuals.extend(target)\n",
        "    return predictions, actuals"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "a5021d0f563546c18e55a9bac0a14a92",
            "ac6016d0a3884d8bb45156217f7a357e",
            "fe234a809f5341f1a832f88ced90e0fd",
            "abed7f80048b495a8a1b9287edb08cf1",
            "787bde7b73c04ca485a7d3ffac6dee48",
            "96cdfaae8c92406e900a0a5b48f20e0f",
            "ef5716710a9b4a68b4cc91ad5c581dbf",
            "6e37979905c749cd9c97efd250859922",
            "af8b524fc9f94a2ca9e83ad3abbc8da8",
            "23d9362b100f4496afcd9a754c465dd0",
            "bbef1db670e341dba6f17abdf2fe7949",
            "65dc0d3d7fe844bc9e9df4a441fc200d",
            "cc053aa33e30447cb16c7a0af0699dd0",
            "a9eb0c685a3749318ac8a4ee67fe774b",
            "88c2715850dd4c6890105b730409df49",
            "5097d2422530404f87dd13691cfa9bfb"
          ]
        },
        "id": "QNWlbGwSGRQn",
        "outputId": "14d9f6ee-dd51-4a55-b91d-52723415835a"
      },
      "source": [
        "def main():\n",
        "    # WandB – Initialize a new run\n",
        "    wandb.init(project=\"transformers_tutorials_summarization\")\n",
        "\n",
        "    # WandB – Config is a variable that holds and saves hyperparameters and inputs\n",
        "    # Defining some key variables that will be used later on in the training  \n",
        "    # config = wandb.config          # Initialize config\n",
        "    # config.TRAIN_BATCH_SIZE = 2    # input batch size for training (default: 64)\n",
        "    # config.VALID_BATCH_SIZE = 2    # input batch size for testing (default: 1000)\n",
        "    # config.TRAIN_EPOCHS = 2        # number of epochs to train (default: 10)\n",
        "    # config.VAL_EPOCHS = 1 \n",
        "    # config.LEARNING_RATE = 1e-4    # learning rate (default: 0.01)\n",
        "    # config.SEED = 42               # random seed (default: 42)\n",
        "    # config.MAX_LEN = 512\n",
        "    # config.SUMMARY_LEN = 150 \n",
        "\n",
        "    # WandB – Config is a variable that holds and saves hyperparameters and inputs\n",
        "    # Defining some key variables that will be used later on in the training  \n",
        "    config = wandb.config          # Initialize config\n",
        "    config.TRAIN_BATCH_SIZE = 5    # input batch size for training (default: 64)\n",
        "    config.VALID_BATCH_SIZE = 5    # input batch size for testing (default: 1000)\n",
        "    config.TRAIN_EPOCHS = 30        # number of epochs to train (default: 10)\n",
        "    config.VAL_EPOCHS = 1 \n",
        "    config.LEARNING_RATE = 1e-4    # learning rate (default: 0.01)\n",
        "    config.SEED = 42               # random seed (default: 42)\n",
        "    config.MAX_LEN = 512\n",
        "    config.SUMMARY_LEN = 150 \n",
        "\n",
        "    # Set random seeds and deterministic pytorch for reproducibility\n",
        "    torch.manual_seed(config.SEED) # pytorch random seed\n",
        "    np.random.seed(config.SEED) # numpy random seed\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "    # tokenzier for encoding the text\n",
        "    tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
        "    \n",
        "\n",
        "    # Importing and Pre-Processing the domain data\n",
        "    # Selecting the needed columns only. \n",
        "    # Adding the summarzie text in front of the text. This is to format the dataset similar to how T5 model was trained for summarization task. \n",
        "    # df = pd.read_csv('/content/drive/Shareddrives/PFE_CHECK/Generation/news_summary.csv',encoding='latin-1')\n",
        "    # df = pd.read_csv('/content/drive/Shareddrives/PFE_CHECK/Generation/news_summary_2.csv',encoding='latin-1')\n",
        "    df = pd.read_csv('/content/drive/Shareddrives/PFE_CHECK/Generation/BDD_csv.csv',encoding='latin-1')\n",
        "    df = df[['text','ctext']]\n",
        "    df.ctext = 'summarize: ' + df.ctext\n",
        "    print(df.head())\n",
        "\n",
        "    \n",
        "    # Creation of Dataset and Dataloader\n",
        "    # Defining the train size. So 80% of the data will be used for training and the rest will be used for validation. \n",
        "    train_size = 0.8\n",
        "    train_dataset=df.sample(frac=train_size,random_state = config.SEED)\n",
        "    val_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n",
        "    train_dataset = train_dataset.reset_index(drop=True)\n",
        "\n",
        "    print(\"FULL Dataset: {}\".format(df.shape))\n",
        "    print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
        "    print(\"TEST Dataset: {}\".format(val_dataset.shape))\n",
        "\n",
        "\n",
        "    # Creating the Training and Validation dataset for further creation of Dataloader\n",
        "    training_set = CustomDataset(train_dataset, tokenizer, config.MAX_LEN, config.SUMMARY_LEN)\n",
        "    val_set = CustomDataset(val_dataset, tokenizer, config.MAX_LEN, config.SUMMARY_LEN)\n",
        "\n",
        "    # Defining the parameters for creation of dataloaders\n",
        "    train_params = {\n",
        "        'batch_size': config.TRAIN_BATCH_SIZE,\n",
        "        'shuffle': True,\n",
        "        'num_workers': 0\n",
        "        }\n",
        "\n",
        "    val_params = {\n",
        "        'batch_size': config.VALID_BATCH_SIZE,\n",
        "        'shuffle': False,\n",
        "        'num_workers': 0\n",
        "        }\n",
        "\n",
        "    # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n",
        "    training_loader = DataLoader(training_set, **train_params)\n",
        "    val_loader = DataLoader(val_set, **val_params)\n",
        "\n",
        "\n",
        "    \n",
        "    # Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary. \n",
        "    # Further this model is sent to device (GPU/TPU) for using the hardware.\n",
        "    # model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
        "    model = T5ForConditionalGeneration.from_pretrained(\"/content/drive/Shareddrives/PFE_CHECK/Generation/test_model_T5_tuto\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Defining the optimizer that will be used to tune the weights of the network in the training session. \n",
        "    optimizer = torch.optim.Adam(params =  model.parameters(), lr=config.LEARNING_RATE)\n",
        "\n",
        "    # Log metrics with wandb\n",
        "    wandb.watch(model, log=\"all\")\n",
        "    # Training loop\n",
        "    print('Initiating Fine-Tuning for the model on our dataset')\n",
        "\n",
        "    for epoch in range(config.TRAIN_EPOCHS):\n",
        "        train(epoch, tokenizer, model, device, training_loader, optimizer)\n",
        "\n",
        "\n",
        "    # Validation loop and saving the resulting file with predictions and acutals in a dataframe.\n",
        "    # Saving the dataframe as predictions.csv\n",
        "    print('Now generating summaries on our fine tuned model for the validation dataset and saving it in a dataframe')\n",
        "    for epoch in range(config.VAL_EPOCHS):\n",
        "        predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n",
        "        final_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})\n",
        "        final_df.to_csv('/content/drive/Shareddrives/PFE_CHECK/Generation/predictions_final_model.csv')\n",
        "        print('Output Files generated for review')\n",
        "\n",
        "    model.save_pretrained(\"/content/drive/Shareddrives/PFE_CHECK/Generation/T5_summarization_model\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdenatflore\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.30<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">glamorous-spaceship-14</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/denatflore/transformers_tutorials_summarization\" target=\"_blank\">https://wandb.ai/denatflore/transformers_tutorials_summarization</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/denatflore/transformers_tutorials_summarization/runs/3l24xjv5\" target=\"_blank\">https://wandb.ai/denatflore/transformers_tutorials_summarization/runs/3l24xjv5</a><br/>\n",
              "                Run data is saved locally in <code>/content/wandb/run-20210520_230022-3l24xjv5</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a5021d0f563546c18e55a9bac0a14a92",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=791656.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "af8b524fc9f94a2ca9e83ad3abbc8da8",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1389353.0, style=ProgressStyle(descript…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "                                                text                                              ctext\n",
            "0  I went storage, cafeteria and electrical. I cr...  summarize: I walked the floor of storage. I ha...\n",
            "1  I started with my upper engine task, I crossed...  summarize: I entered upper engine. I finished ...\n",
            "2  I did my cafeteria task, then I was with Orang...  summarize: I saw Lime, Green, Yellow, Blue and...\n",
            "3  I walked around upper engine with Blue, Lime a...  summarize: I was walking around in the upper e...\n",
            "4  The life of me I spent storage, admin and cafe...  summarize: The life of me was in storage. Oran...\n",
            "FULL Dataset: (10, 2)\n",
            "TRAIN Dataset: (8, 2)\n",
            "TEST Dataset: (2, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Initiating Fine-Tuning for the model on our dataset\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, Loss:  3.575972080230713\n",
            "Epoch: 1, Loss:  2.8980917930603027\n",
            "Epoch: 2, Loss:  2.386510133743286\n",
            "Epoch: 3, Loss:  2.0290820598602295\n",
            "Epoch: 4, Loss:  1.7565603256225586\n",
            "Epoch: 5, Loss:  1.7417837381362915\n",
            "Epoch: 6, Loss:  1.70562744140625\n",
            "Epoch: 7, Loss:  1.2801822423934937\n",
            "Epoch: 8, Loss:  1.0801310539245605\n",
            "Epoch: 9, Loss:  1.2471195459365845\n",
            "Epoch: 10, Loss:  1.1724088191986084\n",
            "Epoch: 11, Loss:  1.0294770002365112\n",
            "Epoch: 12, Loss:  0.7718218564987183\n",
            "Epoch: 13, Loss:  0.8742697238922119\n",
            "Epoch: 14, Loss:  0.6774073243141174\n",
            "Epoch: 15, Loss:  0.5086314082145691\n",
            "Epoch: 16, Loss:  0.5021819472312927\n",
            "Epoch: 17, Loss:  0.46938905119895935\n",
            "Epoch: 18, Loss:  0.45980796217918396\n",
            "Epoch: 19, Loss:  0.3837950825691223\n",
            "Epoch: 20, Loss:  0.34843307733535767\n",
            "Epoch: 21, Loss:  0.33460021018981934\n",
            "Epoch: 22, Loss:  0.25399285554885864\n",
            "Epoch: 23, Loss:  0.24646000564098358\n",
            "Epoch: 24, Loss:  0.17885980010032654\n",
            "Epoch: 25, Loss:  0.19206884503364563\n",
            "Epoch: 26, Loss:  0.2746170163154602\n",
            "Epoch: 27, Loss:  0.16501252353191376\n",
            "Epoch: 28, Loss:  0.14190632104873657\n",
            "Epoch: 29, Loss:  0.13690638542175293\n",
            "Now generating summaries on our fine tuned model for the validation dataset and saving it in a dataframe\n",
            "Completed 0\n",
            "Output Files generated for review\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUrzJa4_a1Bd"
      },
      "source": [
        "# TESTING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "575JWhYka8O9",
        "outputId": "a674dc9c-cb5b-4f21-a37e-d6870a621f21"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SbFKXlPa2TH",
        "outputId": "a5b9fdfb-5bca-4fa7-ac3b-1a4b7cfd5cbf"
      },
      "source": [
        "!pip install --upgrade pip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pip\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/6f/43037c7bcc8bd8ba7c9074256b1a11596daa15555808ec748048c1507f08/pip-21.1.1-py3-none-any.whl (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6MB 4.0MB/s \n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Found existing installation: pip 19.3.1\n",
            "    Uninstalling pip-19.3.1:\n",
            "      Successfully uninstalled pip-19.3.1\n",
            "Successfully installed pip-21.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fz0J3uVCa_RK",
        "outputId": "983f5045-c2cd-4a66-fb11-0e463b3443e7"
      },
      "source": [
        "!pip install transformers[sentencepiece]\n",
        "!pip install wandb -q"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers[sentencepiece]\n",
            "  Downloading transformers-4.6.1-py3-none-any.whl (2.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 4.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (4.0.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (2019.12.20)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 40.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (3.0.12)\n",
            "Collecting huggingface-hub==0.0.8\n",
            "  Downloading huggingface_hub-0.0.8-py3-none-any.whl (34 kB)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 40.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (20.9)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (3.12.4)\n",
            "Collecting sentencepiece==0.1.91\n",
            "  Downloading sentencepiece-0.1.91-cp37-cp37m-manylinux1_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 43.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers[sentencepiece]) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers[sentencepiece]) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers[sentencepiece]) (2.4.7)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->transformers[sentencepiece]) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf->transformers[sentencepiece]) (56.1.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (2020.12.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers[sentencepiece]) (8.0.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers[sentencepiece]) (1.0.1)\n",
            "Installing collected packages: tokenizers, sacremoses, huggingface-hub, transformers, sentencepiece\n",
            "Successfully installed huggingface-hub-0.0.8 sacremoses-0.0.45 sentencepiece-0.1.91 tokenizers-0.10.2 transformers-4.6.1\n",
            "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 4.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 131 kB 40.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 97 kB 6.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 166 kB 37.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.5 MB/s \n",
            "\u001b[?25h  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDV8UW-mbCma"
      },
      "source": [
        "# Importing stock libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Importing the T5 modules from huggingface/transformers\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "# WandB – Import the wandb library\n",
        "import wandb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdtc7gKtbF6X",
        "outputId": "e9ef0ed3-a1a6-4695-8447-3d64fb6534b0"
      },
      "source": [
        "# Checking out the GPU we have access to. This is output is from the google colab version. \n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu May 20 15:14:58 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   53C    P8    32W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5zrGSyQbJ_-"
      },
      "source": [
        "# # Setting up the device for GPU usage\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "\n",
        "# Preparing for TPU usage\n",
        "# import torch_xla\n",
        "# import torch_xla.core.xla_model as xm\n",
        "# device = xm.xla_device()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnZgCZNqPPN3"
      },
      "source": [
        "# Creating a custom dataset for reading the dataframe and loading it into the dataloader to pass it to the neural network at a later stage for finetuning the model and to prepare it for predictions\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataframe, tokenizer, source_len, summ_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.source_len = source_len\n",
        "        self.summ_len = summ_len\n",
        "        self.text = self.data.text\n",
        "        self.ctext = self.data.ctext\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        ctext = str(self.ctext[index])\n",
        "        ctext = ' '.join(ctext.split())\n",
        "\n",
        "        text = str(self.text[index])\n",
        "        text = ' '.join(text.split())\n",
        "\n",
        "        source = self.tokenizer.batch_encode_plus([ctext], max_length= self.source_len, pad_to_max_length=True,return_tensors='pt')\n",
        "        target = self.tokenizer.batch_encode_plus([text], max_length= self.summ_len, pad_to_max_length=True,return_tensors='pt')\n",
        "\n",
        "        source_ids = source['input_ids'].squeeze()\n",
        "        source_mask = source['attention_mask'].squeeze()\n",
        "        target_ids = target['input_ids'].squeeze()\n",
        "        target_mask = target['attention_mask'].squeeze()\n",
        "\n",
        "        return {\n",
        "            'source_ids': source_ids.to(dtype=torch.long), \n",
        "            'source_mask': source_mask.to(dtype=torch.long), \n",
        "            'target_ids': target_ids.to(dtype=torch.long),\n",
        "            'target_ids_y': target_ids.to(dtype=torch.long)\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHAyOBbtblDt"
      },
      "source": [
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
        "\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"/content/drive/Shareddrives/PFE_CHECK/Generation/T5_summarization_model\")\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiLlDqL3byzo"
      },
      "source": [
        "# def summarize(text, max_length=50):\n",
        "#   input_ids = tokenizer.encode(text, return_tensors=\"pt\", add_special_tokens=True)\n",
        "\n",
        "#   generated_ids = model.generate(input_ids=input_ids, num_beams=2, max_length=max_length,  repetition_penalty=2.5, length_penalty=1.0, early_stopping=True)\n",
        "\n",
        "#   preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
        "\n",
        "#   return preds[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ox7FFsT-HBLe"
      },
      "source": [
        "text = \"\"\"I walked the floor of storage. I have walked the floor of cafeteria. I entered storage. I entered electrical. I entered storage. I saw Black. I saw more Black. I entered electrical. I have finished the Calibrate Distributor task. I entered storage. I entered shields. I entered oxygen. I entered weapons. I entered cafeteria. I entered admin. I saw Blue. I saw more Blue. I finished the Swipe Card task. I walked the floor of storage. I walked the floor of electrical. I have finished the Calibrate Distributor task. I entered storage. I have walked on shields soil. I entered oxygen. I have walked on shields soil. I walked the floor of storage. I walked the floor of electrical. I have finished the Calibrate Distributor task. I walked the floor of storage. I entered shields. I entered oxygen. I entered shields. I walked the floor of storage. I walked the floor of electrical. I have finished the Calibrate Distributor task. I finished the Divert Power task. I entered lower engine. I have stepped on the floor of security. I saw Green. I saw more Green. I finished the Accept Power (security) task.\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LaOe90s5izy"
      },
      "source": [
        "def generate(tokenizer, model, device, text):\n",
        "    MAX_LEN = 512\n",
        "    SUMMARY_LEN = 50 \n",
        "    VALID_BATCH_SIZE = 5\n",
        "\n",
        "    val_dataset= pd.DataFrame([[\"\",text]],columns=[\"text\",\"ctext\"])\n",
        "\n",
        "    val_set = CustomDataset(val_dataset, tokenizer, MAX_LEN, SUMMARY_LEN)\n",
        "\n",
        "    val_params = {\n",
        "            'batch_size': VALID_BATCH_SIZE,\n",
        "            'shuffle': False,\n",
        "            'num_workers': 0\n",
        "            }\n",
        "\n",
        "    loader = DataLoader(val_set, **val_params)\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "    with torch.no_grad():\n",
        "        for _, data in enumerate(loader, 0):\n",
        "            ids = data['source_ids'].to(device, dtype = torch.long)\n",
        "            mask = data['source_mask'].to(device, dtype = torch.long)\n",
        "\n",
        "            generated_ids = model.generate(\n",
        "                input_ids = ids,\n",
        "                attention_mask = mask, \n",
        "                max_length=150, \n",
        "                num_beams=2,\n",
        "                repetition_penalty=2.5, \n",
        "                length_penalty=1.0, \n",
        "                early_stopping=True\n",
        "                )\n",
        "            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
        "\n",
        "            predictions.extend(preds)\n",
        "    return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "E0dSpc1-5z49",
        "outputId": "48fa134c-59d8-4062-e704-aae1d04d0c9a"
      },
      "source": [
        "generate(tokenizer, model, device, text)[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'storage, cafeteria and electrical. I crossed Black in storage and Blue in admin while going to do Swipe Card task. At the end I was doing my tasks at lower engine then electrical and I crossed paths with Green. Arnaud is suspect.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLNZvCsON1qs"
      },
      "source": [
        "text = 'storage, cafeteria and electrical. I crossed Black in storage and Blue in admin while going to do Swipe Card task. At the end I was doing my tasks at lower engine then electrical and I crossed paths with Green. Arnaud is suspect. test pb fin'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRyaakaGO5o4",
        "outputId": "2c8c3049-f412-4601-ea23-21ca2cc0590d"
      },
      "source": [
        "text.split(\".\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['storage, cafeteria and electrical',\n",
              " ' I crossed Black in storage and Blue in admin while going to do Swipe Card task',\n",
              " ' At the end I was doing my tasks at lower engine then electrical and I crossed paths with Green',\n",
              " ' Arnaud is suspect',\n",
              " ' test pb fin']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "SJbBVyAmN5V3",
        "outputId": "e5e418f2-7205-4823-fdf7-f096bbddda62"
      },
      "source": [
        "if text[0].isupper():\n",
        "  newText = \".\".join(text.split(\".\")[:-1])\n",
        "else:\n",
        "  newText = \".\".join(text.split(\".\")[1:-1])\n",
        "newText"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' I crossed Black in storage and Blue in admin while going to do Swipe Card task. At the end I was doing my tasks at lower engine then electrical and I crossed paths with Green. Arnaud is suspect'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tY5JUJqq6GN3"
      },
      "source": [
        "# Score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPKxnnMFZFdB"
      },
      "source": [
        "#rouge scores for a reference/generated sentence pair\n",
        "#source google seq2seq source code.\n",
        "\n",
        "import itertools\n",
        "\n",
        "#supporting function\n",
        "def _split_into_words(sentences):\n",
        "  \"\"\"Splits multiple sentences into words and flattens the result\"\"\"\n",
        "  return list(itertools.chain(*[_.split(\" \") for _ in sentences]))\n",
        "\n",
        "#supporting function\n",
        "def _get_word_ngrams(n, sentences):\n",
        "  \"\"\"Calculates word n-grams for multiple sentences.\n",
        "  \"\"\"\n",
        "  assert len(sentences) > 0\n",
        "  assert n > 0\n",
        "\n",
        "  words = _split_into_words(sentences)\n",
        "  return _get_ngrams(n, words)\n",
        "\n",
        "#supporting function\n",
        "def _get_ngrams(n, text):\n",
        "  \"\"\"Calcualtes n-grams.\n",
        "  Args:\n",
        "    n: which n-grams to calculate\n",
        "    text: An array of tokens\n",
        "  Returns:\n",
        "    A set of n-grams\n",
        "  \"\"\"\n",
        "  ngram_set = set()\n",
        "  text_length = len(text)\n",
        "  max_index_ngram_start = text_length - n\n",
        "  for i in range(max_index_ngram_start + 1):\n",
        "    ngram_set.add(tuple(text[i:i + n]))\n",
        "  return ngram_set\n",
        "\n",
        "def rouge_n(reference_sentences, evaluated_sentences, n=2):\n",
        "  \"\"\"\n",
        "  Computes ROUGE-N of two text collections of sentences.\n",
        "  Source: http://research.microsoft.com/en-us/um/people/cyl/download/\n",
        "  papers/rouge-working-note-v1.3.1.pdf\n",
        "  Args:\n",
        "    evaluated_sentences: The sentences that have been picked by the summarizer\n",
        "    reference_sentences: The sentences from the referene set\n",
        "    n: Size of ngram.  Defaults to 2.\n",
        "  Returns:\n",
        "    recall rouge score(float)\n",
        "  Raises:\n",
        "    ValueError: raises exception if a param has len <= 0\n",
        "  \"\"\"\n",
        "  if len(evaluated_sentences) <= 0 or len(reference_sentences) <= 0:\n",
        "    raise ValueError(\"Collections must contain at least 1 sentence.\")\n",
        "\n",
        "  evaluated_ngrams = _get_word_ngrams(n, evaluated_sentences)\n",
        "  reference_ngrams = _get_word_ngrams(n, reference_sentences)\n",
        "  reference_count = len(reference_ngrams)\n",
        "  evaluated_count = len(evaluated_ngrams)\n",
        "\n",
        "  # Gets the overlapping ngrams between evaluated and reference\n",
        "  overlapping_ngrams = evaluated_ngrams.intersection(reference_ngrams)\n",
        "  overlapping_count = len(overlapping_ngrams)\n",
        "\n",
        "  # Handle edge case. This isn't mathematically correct, but it's good enough\n",
        "  if evaluated_count == 0:\n",
        "    precision = 0.0\n",
        "  else:\n",
        "    precision = overlapping_count / evaluated_count\n",
        "\n",
        "  if reference_count == 0:\n",
        "    recall = 0.0\n",
        "  else:\n",
        "    recall = overlapping_count / reference_count\n",
        "\n",
        "  f1_score = 2.0 * ((precision * recall) / (precision + recall + 1e-8))\n",
        "\n",
        "  #just returning recall count in rouge, useful for our purpose\n",
        "  return recall,precision,f1_score"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnlOsSpk6MOu"
      },
      "source": [
        "# Text and generation from validation space.\n",
        "realText = \"\"\"I walked around upper engine with Blue, Lime and Orange, then I did my cafeteria and storage tasks. I crossed Orange to electrical, then Green to reactor. When I returned to electrical I saw Lime dead, it's Orange for sure. Arnaud is suspect.\"\"\"\n",
        "generated = \"\"\"I crossed Lime's backbone while walking around the upper engine, my heart open to the unknown. At the reactor, I ran into Vert and I crossed Lime's backbone. Then I crossed Lime's backbone. Arnaud is suspect.\"\"\"\n",
        "realText_2 = \"\"\"The life of me I was at Admin, Cafeteria and O2 with my heart open to the unknown. Then I walked on the floor of Security, Lower Engine and Admin. Finaly I walked around Cafeteria, Weapons, O2 and Shields with my heart open to the unknown. Arnaud is suspect.\"\"\"\n",
        "generated_2 = \"\"\"I finished my tasks Clear Asteroids, Shields and Prime Shields. The life of me I was then at Admin and Cafeteria with Blue. Then I crossed Blue at None. I finished my tasks. I entered Medbay, Upper Engine, Reactor, Lower Engine, Electrical and I was at Storage. The life of me I was then at Navigation. I entered Shields, Communications and I entered Storage. The life of me I was then at Navigation. I entered Weapons and I entered O2. The life of me\"\"\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmpaBHrJ6K21",
        "outputId": "84913bcd-ff0b-4598-e597-778aedac4831"
      },
      "source": [
        "recall,precision,f1_score = rouge_n(realText, generated, n=2)\n",
        "print(\"Rouge-2 recall : \" + str(recall))\n",
        "print(\"Rouge-2 precision : \" + str(precision))\n",
        "print(\"Rouge-2 f1_score : \" + str(f1_score))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Rouge-2 recall : 0.5952380952380952\n",
            "Rouge-2 precision : 0.7142857142857143\n",
            "Rouge-2 f1_score : 0.6493506443919718\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MstwIHr16gsP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61ca05c9-2a57-496d-b3f2-bdecef0cb316"
      },
      "source": [
        "recall,precision,f1_score = rouge_n(realText_2, generated_2, n=2)\n",
        "print(\"Rouge-2 recall : \" + str(recall))\n",
        "print(\"Rouge-2 precision : \" + str(precision))\n",
        "print(\"Rouge-2 f1_score : \" + str(f1_score))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Rouge-2 recall : 0.71875\n",
            "Rouge-2 precision : 0.5859872611464968\n",
            "Rouge-2 f1_score : 0.6456140301394891\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SaQCAzuKQmm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}